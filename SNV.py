import numpy as np
import torch
from networkx import eigenvector_centrality

from sklearn.metrics import (
    roc_auc_score, precision_recall_curve, auc,
    f1_score, precision_score, recall_score, accuracy_score
)
import torch.nn as nn
import torch.nn.functional as F
from torch.nn import BCEWithLogitsLoss
from torch_geometric.nn import GCNConv, SAGEConv, GATConv
from torch_geometric.utils import to_dense_adj, dense_to_sparse, degree, coalesce
from UNGSL_test.cluster import create_cluster
from UNGSL_test.data_h5_loader import read_h5file
import pandas as pd
import warnings

warnings.filterwarnings("ignore")

torch.manual_seed(42)
np.random.seed(42)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False


# === æ–°å¢ï¼š4ä¸ªç»„å­¦æ•°æ®æ‹†åˆ†å‡½æ•°ï¼ˆä»…ä½œç”¨äºè¾“å…¥çš„èŠ‚ç‚¹å­é›†ï¼‰===
def SNV_split(node_subset, x_full):
    # ä»…å¯¹node_subsetä¸­çš„èŠ‚ç‚¹ï¼Œç”¨SNVç‰¹å¾ï¼ˆå‰16åˆ—ï¼‰åˆ’åˆ†
    x_np = x_full.cpu().numpy()
    # æå–å­é›†èŠ‚ç‚¹çš„SNVç‰¹å¾
    x_snv_subset = x_np[node_subset, :16]
    row_mean = np.mean(x_snv_subset, axis=1)
    # å¯¹å­é›†èŠ‚ç‚¹æŒ‰å‡å€¼æ’åºï¼ˆç´¢å¼•å¯¹åº”å­é›†å†…éƒ¨ï¼‰
    sorted_subset_idx = np.argsort(row_mean)
    split_point = int(sorted_subset_idx.shape[0] * 0.2)
    # æ˜ å°„å›åŸå§‹èŠ‚ç‚¹IDï¼šæ–°è®­ç»ƒé›†ï¼ˆ80%ï¼‰ã€æ–°éªŒè¯é›†ï¼ˆ20%ï¼‰
    new_train_idx = node_subset[sorted_subset_idx[split_point:]]
    new_val_idx = node_subset[sorted_subset_idx[:split_point]]
    return new_train_idx, new_val_idx


def GE_split(node_subset, x_full):
    # ä»…å¯¹node_subsetä¸­çš„èŠ‚ç‚¹ï¼Œç”¨GEç‰¹å¾ï¼ˆ32-48åˆ—ï¼‰åˆ’åˆ†
    x_np = x_full.cpu().numpy()
    x_ge_subset = x_np[node_subset, 32:48]
    row_mean = np.mean(x_ge_subset, axis=1)
    sorted_subset_idx = np.argsort(row_mean)
    split_point = int(sorted_subset_idx.shape[0] * 0.2)
    new_train_idx = node_subset[sorted_subset_idx[split_point:]]
    new_val_idx = node_subset[sorted_subset_idx[:split_point]]
    return new_train_idx, new_val_idx


def METH_split(node_subset, x_full):
    # ä»…å¯¹node_subsetä¸­çš„èŠ‚ç‚¹ï¼Œç”¨METHç‰¹å¾ï¼ˆ16-32åˆ—ï¼‰åˆ’åˆ†
    x_np = x_full.cpu().numpy()
    x_meth_subset = x_np[node_subset, 16:32]
    row_mean = np.mean(x_meth_subset, axis=1)
    sorted_subset_idx = np.argsort(row_mean)
    split_point = int(sorted_subset_idx.shape[0] * 0.2)
    new_train_idx = node_subset[sorted_subset_idx[split_point:]]
    new_val_idx = node_subset[sorted_subset_idx[:split_point]]
    return new_train_idx, new_val_idx


def CNA_split(node_subset, x_full):
    # ä»…å¯¹node_subsetä¸­çš„èŠ‚ç‚¹ï¼Œç”¨CNAç‰¹å¾ï¼ˆ32åˆ—åï¼‰åˆ’åˆ†
    x_np = x_full.cpu().numpy()
    x_cna_subset = x_np[node_subset, 48:]
    row_mean = np.mean(x_cna_subset, axis=1)
    sorted_subset_idx = np.argsort(row_mean)
    split_point = int(sorted_subset_idx.shape[0] * 0.2)
    new_train_idx = node_subset[sorted_subset_idx[split_point:]]
    new_val_idx = node_subset[sorted_subset_idx[:split_point]]
    return new_train_idx, new_val_idx


# === è®¾å¤‡ä¸æ•°æ®åŠ è½½ï¼ˆå®Œå…¨ä¿ç•™åŸå§‹æ•°æ®é›†çš„train/val/teståˆ’åˆ†ï¼Œåç»­ä»…è°ƒæ•´è®­ç»ƒé›†å†…éƒ¨ï¼‰===
primary_device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

data = read_h5file("networks/LTG_multiomics.h5")
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
x_np = data.x.cpu().numpy()
x_scaled = scaler.fit_transform(x_np)
data.x = torch.tensor(x_scaled, dtype=torch.float)
data_full = data.to(primary_device)
data_full.y = data_full.y.float()
dataset = create_cluster(data.cpu())

# æ‰“å°åŸå§‹æ•°æ®é›†åŸºæœ¬ä¿¡æ¯ï¼ˆç¡®è®¤åŸå§‹åˆ’åˆ†æœªè¢«ä¿®æ”¹ï¼‰
print("=== åŸå§‹æ•°æ®é›†ä¿¡æ¯ ===")
for i, sub in enumerate(dataset):
    pos_ratio = sub.y.float().mean().item()
    print(f"[Subgraph {i}] Nodes: {sub.num_nodes}, Pos ratio: {pos_ratio:.4f}")
print(f"åŸå§‹è®­ç»ƒé›†èŠ‚ç‚¹æ•°: {data_full.train_mask.sum().item()}")
print(f"åŸå§‹éªŒè¯é›†èŠ‚ç‚¹æ•°: {data_full.val_mask.sum().item()}")
print(f"åŸå§‹æµ‹è¯•é›†èŠ‚ç‚¹æ•°: {data_full.test_mask.sum().item()}")

config = {
    'gsl_hidden': 256,
    'cls_hidden': 128,
    'dropout': 0.5,
    'threshold': 0.01,
    'alpha': 0.8,
    'reg_weight': 1e-5,
    'topk': 10,
    'top_p': 0.1,
    'aff_weight': 0.1,
}


# === æ¨¡å‹å®šä¹‰ï¼ˆä¿æŒä¸å˜ï¼‰===
class EnhancedGSL(nn.Module):
    def __init__(self, in_channel, hidden_channel):
        super().__init__()
        self.mlp = nn.Sequential(
            nn.Linear(in_channel, hidden_channel),
            nn.BatchNorm1d(hidden_channel),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(hidden_channel, hidden_channel),
        )

    def forward(self, x, original_adj):
        x = self.mlp(x)
        x = F.normalize(x, p=2, dim=1)
        sim = torch.mm(x, x.t())
        adj = config['alpha'] * sim + (1 - config['alpha']) * original_adj
        return adj


class EnhancedClassifier(nn.Module):
    def __init__(self, in_channel, hidden_channel, out_channel):
        super().__init__()
        self.conv1 = SAGEConv(in_channel, hidden_channel)
        self.bn1 = nn.LayerNorm(hidden_channel)
        self.conv2 = SAGEConv(hidden_channel, hidden_channel)
        self.conv3 = SAGEConv(hidden_channel, hidden_channel)
        self.dropout = config['dropout']
        self.conv4 = SAGEConv(hidden_channel, out_channel)
        self.residual = nn.Linear(in_channel, hidden_channel) if in_channel != hidden_channel else None

    def forward(self, x, edge_index):
        identity = x
        x = F.relu(self.bn1(self.conv1(x, edge_index)))
        if self.residual is not None:
            x = x + self.residual(identity)
        x = F.dropout(x, p=self.dropout, training=self.training)
        x = F.relu(self.conv2(x, edge_index))
        x = F.dropout(x, p=self.dropout, training=self.training)
        x = F.relu(self.conv3(x, edge_index))
        x = F.dropout(x, p=self.dropout, training=self.training)
        x = self.conv4(x, edge_index)
        return x.squeeze(-1)


class UNGSLayer(nn.Module):
    def __init__(self, num_nodes):
        super().__init__()
        self.conf_weight = nn.Parameter(torch.ones(num_nodes) * 0.5)

    def forward(self, s, confidence):
        conf_mat = (confidence.unsqueeze(0) + confidence.unsqueeze(1)) / 2
        adj = torch.sigmoid(s) * torch.sigmoid(conf_mat)
        adj = (adj + adj.t()) / 2
        return adj


class FullGraphGNN(nn.Module):
    def __init__(self, in_dim, hidden_dim, out_dim=1):
        super().__init__()
        self.gat = GATConv(
            in_channels=in_dim,
            out_channels=hidden_dim // 2,
            heads=1,  # å‡å°‘å¤´æ•°ä»¥èŠ‚çœæ˜¾å­˜
            concat=False,
            dropout=0.3,
            add_self_loops=True
        )
        self.sage = SAGEConv(in_dim, hidden_dim // 2)
        self.bn1 = nn.LayerNorm(hidden_dim)
        self.conv2 = SAGEConv(hidden_dim, hidden_dim)
        self.bn2 = nn.LayerNorm(hidden_dim)
        self.classifier = nn.Linear(hidden_dim, out_dim)
        self.dropout = nn.Dropout(0.5)

    def forward(self, x, edge_index, edge_weight=None):
        x1 = self.gat(x, edge_index)
        x2 = self.sage(x, edge_index, edge_weight)
        x = torch.cat([x1, x2], dim=-1)
        x = F.relu(self.bn1(x))
        x = self.dropout(x)
        x = self.conv2(x, edge_index, edge_weight)
        x = F.relu(self.bn2(x))
        x = self.dropout(x)
        x = self.classifier(x)
        return x.squeeze(-1)


class FocalLoss(nn.Module):
    def __init__(self, alpha=0.75, gamma=2.0):
        super().__init__()
        self.alpha = alpha
        self.gamma = gamma

    def forward(self, logits, targets):
        bce = F.binary_cross_entropy_with_logits(logits, targets, reduction='none')
        pt = torch.exp(-bce)
        loss = self.alpha * (1 - pt) ** self.gamma * bce
        return loss.mean()


def compute_confidence(logits):
    prob = torch.sigmoid(logits)
    entropy = - (prob * torch.log(prob + 1e-10) + (1 - prob) * torch.log(1 - prob + 1e-10))
    confidence = 1.0 - entropy
    return confidence.clamp(0.0, 1.0)


# === å­å›¾è®­ç»ƒå‡½æ•°ï¼ˆæ˜¾å­˜ä¼˜åŒ–ï¼Œä¿æŒä¸å˜ï¼‰===
def train_model(subgraph):
    device = primary_device
    subgraph = subgraph.to(device)
    n = subgraph.num_nodes
    x = subgraph.x
    y = subgraph.y.float()
    edge_index = subgraph.edge_index
    x = x / (x.norm(dim=1, keepdim=True) + 1e-8)
    adj_dense = to_dense_adj(edge_index, max_num_nodes=n)[0].to(device)

    pretrain_gsl = EnhancedGSL(subgraph.num_features, config['gsl_hidden']).to(device)
    pretrain_cls = EnhancedClassifier(subgraph.num_features, config['cls_hidden'], 1).to(device)
    pretrain_optim = torch.optim.AdamW([
        {'params': pretrain_gsl.parameters(), 'lr': 1e-3, 'weight_decay': 1e-3},
        {'params': pretrain_cls.parameters(), 'lr': 1e-3, 'weight_decay': 1e-4}
    ])
    criterion = nn.BCEWithLogitsLoss()

    def affinity_loss(S, y, margin=0.5):
        y = y.float()
        pos_mask = (y.unsqueeze(0) * y.unsqueeze(1)) > 0
        neg_mask = (y.unsqueeze(0) + y.unsqueeze(1)) == 0
        loss = 0.0
        if pos_mask.any():
            loss += F.relu(margin - S[pos_mask]).mean()
        if neg_mask.any():
            loss += F.relu(S[neg_mask] - margin).mean()
        return loss

    for epoch in range(1, 151):
        pretrain_gsl.train()
        pretrain_cls.train()
        pretrain_optim.zero_grad()
        S = pretrain_gsl(x, adj_dense)
        aff_loss = config['aff_weight'] * affinity_loss(S, y)
        edge_index_S = torch.nonzero(S > config['threshold']).t()
        logits = pretrain_cls(x, edge_index_S).squeeze(-1)
        cls_loss = criterion(logits, y)
        loss = cls_loss + aff_loss
        loss.backward()
        torch.nn.utils.clip_grad_norm_(pretrain_gsl.parameters(), max_norm=2.0)
        torch.nn.utils.clip_grad_norm_(pretrain_cls.parameters(), max_norm=2.0)
        pretrain_optim.step()

    model_gsl = EnhancedGSL(subgraph.num_features, config['gsl_hidden']).to(device)
    model_cls = EnhancedClassifier(subgraph.num_features, config['cls_hidden'], 1).to(device)
    model_ungsl = UNGSLayer(n).to(device)
    model_gsl.load_state_dict(pretrain_gsl.state_dict())
    model_cls.load_state_dict(pretrain_cls.state_dict())

    model_cls.requires_grad_(False)
    finetune_optim = torch.optim.AdamW([
        {'params': model_gsl.parameters(), 'lr': 1e-3},
        {'params': model_ungsl.parameters(), 'lr': 5e-3}
    ], weight_decay=1e-5)

    for epoch in range(1, 201):
        model_gsl.train()
        model_ungsl.train()
        if epoch == 100:
            model_cls.requires_grad_(True)
            finetune_optim = torch.optim.AdamW([
                {'params': model_gsl.parameters(), 'lr': 1e-4},
                {'params': model_ungsl.parameters(), 'lr': 5e-4},
                {'params': model_cls.parameters(), 'lr': 1e-4, 'weight_decay': 1e-4}
            ], weight_decay=1e-5)
        finetune_optim.zero_grad()
        S = model_gsl(x, adj_dense)
        with torch.no_grad():
            cls_out = model_cls(x, edge_index)
            confidence = compute_confidence(cls_out)
        S_hat = model_ungsl(S, confidence)
        edge_index_finetune = torch.nonzero(S_hat > config['threshold']).t()
        out = model_cls(x, edge_index_finetune).squeeze(-1)
        cls_loss = criterion(out, y)
        reg_loss = torch.norm(S_hat, p=1)
        reg_weight = config['reg_weight'] * (1 + epoch / 500)
        total_loss = cls_loss + reg_weight * reg_loss
        total_loss.backward()
        finetune_optim.step()

    # æ˜¾å­˜é‡Šæ”¾
    orig_idx = subgraph.orig_node_idx.clone()
    del adj_dense, S, S_hat, edge_index_S, edge_index_finetune, subgraph
    torch.cuda.empty_cache()
    return model_gsl, model_cls, model_ungsl, orig_idx


# === Step 1: è®­ç»ƒæ‰€æœ‰å­å›¾æ¨¡å‹ï¼ˆä¿æŒä¸å˜ï¼‰===
models = []
for sub in dataset:
    print(f"\nTraining subgraph {len(models) + 1}/{len(dataset)}...")
    models.append(train_model(sub))

# === Step 2: ç»“æ„é›†æˆï¼ˆç¨€ç–åŒ–ï¼Œä¿æŒä¸å˜ï¼‰===
N = data_full.num_nodes
all_edges = []
all_weights = []

for i, (model_gsl, model_cls, model_ungsl, orig_idx) in enumerate(models):
    print(f"\nProcessing subgraph {i + 1}/{len(models)} for structure ensemble...")
    model_gsl.eval()
    model_ungsl.eval()
    model_cls.eval()
    subgraph = dataset[i].to(primary_device)
    sub_nodes = orig_idx.to(primary_device)
    n_sub = sub_nodes.size(0)
    global_to_local = torch.full((N,), -1, dtype=torch.long, device=primary_device)
    global_to_local[sub_nodes] = torch.arange(n_sub, device=primary_device)
    edge_index_global = subgraph.edge_index.to(primary_device)
    edge_index_local = global_to_local[edge_index_global]
    edge_index_local = edge_index_local[:, (edge_index_local[0] >= 0) & (edge_index_local[1] >= 0)]
    adj_sub = to_dense_adj(edge_index_local, max_num_nodes=n_sub)[0]
    x_sub = subgraph.x.to(primary_device)
    with torch.no_grad():
        S_sub = model_gsl(x_sub, adj_sub)
        cls_out_sub = model_cls(x_sub, edge_index_local).squeeze(-1)
        confidence_sub = compute_confidence(cls_out_sub)
        S_hat_sub = model_ungsl(S_sub, confidence_sub)

        triu_mask = torch.triu(torch.ones_like(S_hat_sub), diagonal=1).bool()
        values = S_hat_sub[triu_mask]
        if values.numel() > 0:
            k = max(5, int(config['top_p'] * values.numel()))
            if k < values.numel():
                threshold_val = torch.kthvalue(values, values.numel() - k + 1).values
            else:
                threshold_val = values.min()
            mask = S_hat_sub >= threshold_val
            mask = mask | mask.t()
        else:
            mask = torch.zeros_like(S_hat_sub, dtype=torch.bool)

        rows, cols = torch.nonzero(mask, as_tuple=True)
        if rows.numel() > 0:
            global_rows = sub_nodes[rows]
            global_cols = sub_nodes[cols]
            weights = S_hat_sub[rows, cols]
            all_edges.append(torch.stack([global_rows, global_cols], dim=0))
            all_weights.append(weights)

    # æ˜¾å­˜æ¸…ç†
    del S_sub, S_hat_sub, mask, adj_sub, x_sub, subgraph
    torch.cuda.empty_cache()

# åˆå¹¶æ‰€æœ‰è¾¹ï¼ˆå»é‡ + æƒé‡èšåˆï¼‰
if all_edges:
    edge_index_ensemble = torch.cat(all_edges, dim=1)
    edge_weight_ensemble = torch.cat(all_weights, dim=0)
    # å»é‡ï¼šä¿ç•™æœ€å¤§æƒé‡
    edge_index_ensemble, inverse = torch.unique(edge_index_ensemble, dim=1, return_inverse=True)
    final_weights = torch.zeros(edge_index_ensemble.size(1), device=primary_device)
    final_weights.scatter_reduce_(0, inverse, edge_weight_ensemble, reduce="max", include_self=False)
else:
    edge_index_ensemble = torch.empty((2, 0), dtype=torch.long, device=primary_device)
    final_weights = torch.empty(0, device=primary_device)

print(f"\nâœ… Ensemble graph has {edge_index_ensemble.size(1)} edges.")


# === æ ¸å¿ƒï¼šè¯„ä¼°å‡½æ•°ï¼ˆæ”¯æŒä¼ å…¥â€œè®­ç»ƒé›†å†…éƒ¨åˆ’åˆ†çš„æ–°maskâ€ï¼Œå®Œå…¨ä¿ç•™åŸå§‹æµ‹è¯•é›†ï¼‰===
def train_and_evaluate(edge_index, edge_weight, x_full, y_full,
                       new_train_mask, new_val_mask, original_test_mask,  # å…³é”®ï¼šç”¨æ–°è®­ç»ƒ/éªŒè¯maskï¼ŒåŸå§‹æµ‹è¯•mask
                       pos_weight, device, model_save_path,
                       epochs=500, lr=0.01, weight_decay=5e-4,
                       max_patience=30):
    model = FullGraphGNN(in_dim=x_full.size(1), hidden_dim=64).to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=10)
    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight.to(device))

    best_val_aupr = 0.0
    patience = 0

    for epoch in range(epochs):
        model.train()
        optimizer.zero_grad()
        # è®­ç»ƒï¼šä»…ç”¨â€œè®­ç»ƒé›†å†…éƒ¨åˆ’åˆ†çš„æ–°è®­ç»ƒé›†â€
        out = model(x_full, edge_index, edge_weight)
        loss = criterion(out[new_train_mask], y_full[new_train_mask])
        loss.backward()
        optimizer.step()

        model.eval()
        with torch.no_grad():
            # éªŒè¯ï¼šä»…ç”¨â€œè®­ç»ƒé›†å†…éƒ¨åˆ’åˆ†çš„æ–°éªŒè¯é›†â€
            val_out = model(x_full, edge_index, edge_weight)
            val_probs = torch.sigmoid(val_out[new_val_mask]).cpu().numpy()
            val_labels = y_full[new_val_mask].cpu().numpy()

            if val_labels.sum() == 0:
                val_aupr = 0.0
            else:
                precision, recall, _ = precision_recall_curve(val_labels, val_probs)
                val_aupr = auc(recall, precision)

        scheduler.step(val_aupr)

        if val_aupr > best_val_aupr:
            best_val_aupr = val_aupr
            patience = 0
            torch.save(model.state_dict(), model_save_path)
        else:
            patience += 1

        if patience >= max_patience:
            break

    model.load_state_dict(torch.load(model_save_path, map_location=device))
    model.eval()
    with torch.no_grad():
        # æœ€ç»ˆéªŒè¯ï¼ˆé€‰é˜ˆå€¼ï¼‰ï¼šä»ç”¨æ–°éªŒè¯é›†
        val_out = model(x_full, edge_index, edge_weight)
        val_probs = torch.sigmoid(val_out[new_val_mask]).cpu().numpy()
        val_labels = y_full[new_val_mask].cpu().numpy()

        best_thr = 0.5
        if val_labels.sum() > 0:
            precision, recall, thresholds = precision_recall_curve(val_labels, val_probs)
            best_metric = 0.0
            for i, thr in enumerate(thresholds):
                if recall[i] >= 0.60:
                    f1_local = 2 * (precision[i] * recall[i]) / (precision[i] + recall[i] + 1e-8)
                    if f1_local > best_metric:
                        best_metric = f1_local
                        best_thr = thr
            if best_metric == 0.0:
                best_thr = 0.5

        # æµ‹è¯•ï¼šå®Œå…¨ç”¨åŸå§‹æµ‹è¯•é›†ï¼ˆä¸ä¿®æ”¹ï¼‰
        test_out = model(x_full, edge_index, edge_weight)
        test_probs = torch.sigmoid(test_out[original_test_mask]).cpu().numpy()
        test_labels = y_full[original_test_mask].cpu().numpy()

        pred_test = (test_probs > best_thr).astype(int)
        acc = accuracy_score(test_labels, pred_test)
        auc_score = roc_auc_score(test_labels, test_probs) if len(np.unique(test_labels)) > 1 else 0.5
        precision, recall, _ = precision_recall_curve(test_labels, test_probs)
        aupr_score = auc(recall, precision)
        f1 = f1_score(test_labels, pred_test)
        prec = precision_score(test_labels, pred_test) if pred_test.sum() > 0 else 0.0
        rec = recall_score(test_labels, pred_test)

    return {
        'val_aupr': best_val_aupr,
        'test_acc': acc,
        'test_auc': auc_score,
        'test_aupr': aupr_score,
        'test_f1': f1,
        'test_prec': prec,
        'test_rec': rec,
        'best_thr': best_thr
    }


# === æ ¸å¿ƒï¼šåˆ†é˜¶æ®µè¯„ä¼°å‡½æ•°ï¼ˆé€‚é…ç²¾ç‚¼å›¾ï¼ŒåŒæ ·ä¿ç•™åŸå§‹æµ‹è¯•é›†ï¼‰===
def train_and_evaluate_a(edge_index_train, edge_index_infer, x_full, y_full,
                         new_train_mask, new_val_mask, original_test_mask,
                         pos_weight, device, model_save_path,
                         epochs=500, lr=0.01, weight_decay=5e-4,
                         max_patience=30):
    model = FullGraphGNN(in_dim=x_full.size(1), hidden_dim=64).to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=10)
    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight.to(device))

    best_val_aupr = 0.0
    patience = 0

    for epoch in range(epochs):
        model.train()
        optimizer.zero_grad()
        # è®­ç»ƒï¼šç”¨è®­ç»ƒä¸“ç”¨è¾¹ + æ–°è®­ç»ƒé›†
        out = model(x_full, edge_index_train)
        loss = criterion(out[new_train_mask], y_full[new_train_mask])
        loss.backward()
        optimizer.step()

        model.eval()
        with torch.no_grad():
            # éªŒè¯ï¼šç”¨æ¨ç†ä¸“ç”¨è¾¹ + æ–°éªŒè¯é›†
            val_out = model(x_full, edge_index_infer)
            val_probs = torch.sigmoid(val_out[new_val_mask]).cpu().numpy()
            val_labels = y_full[new_val_mask].cpu().numpy()

            if val_labels.sum() == 0:
                val_aupr = 0.0
            else:
                precision, recall, _ = precision_recall_curve(val_labels, val_probs)
                val_aupr = auc(recall, precision)

        scheduler.step(val_aupr)

        if val_aupr > best_val_aupr:
            best_val_aupr = val_aupr
            patience = 0
            torch.save(model.state_dict(), model_save_path)
        else:
            patience += 1

        if patience >= max_patience:
            break

    model.load_state_dict(torch.load(model_save_path, map_location=device))
    model.eval()
    with torch.no_grad():
        # é€‰é˜ˆå€¼ï¼šæ–°éªŒè¯é›†
        val_out = model(x_full, edge_index_infer)
        val_probs = torch.sigmoid(val_out[new_val_mask]).cpu().numpy()
        val_labels = y_full[new_val_mask].cpu().numpy()

        best_thr = 0.5
        if val_labels.sum() > 0:
            precision, recall, thresholds = precision_recall_curve(val_labels, val_probs)
            best_metric = 0.0
            for i, thr in enumerate(thresholds):
                if recall[i] >= 0.60:
                    f1_local = 2 * (precision[i] * recall[i]) / (precision[i] + recall[i] + 1e-8)
                    if f1_local > best_metric:
                        best_metric = f1_local
                        best_thr = thr
            if best_metric == 0.0:
                best_thr = 0.5

        # æµ‹è¯•ï¼šåŸå§‹æµ‹è¯•é›†
        test_out = model(x_full, edge_index_infer)
        test_probs = torch.sigmoid(test_out[original_test_mask]).cpu().numpy()
        test_labels = y_full[original_test_mask].cpu().numpy()

        pred_test = (test_probs > best_thr).astype(int)
        acc = accuracy_score(test_labels, pred_test)
        auc_score = roc_auc_score(test_labels, test_probs) if len(np.unique(test_labels)) > 1 else 0.5
        precision, recall, _ = precision_recall_curve(test_labels, test_probs)
        aupr_score = auc(recall, precision)
        f1 = f1_score(test_labels, pred_test)
        prec = precision_score(test_labels, pred_test) if pred_test.sum() > 0 else 0.0
        rec = recall_score(test_labels, pred_test)

    return {
        'val_aupr': best_val_aupr,
        'test_acc': acc,
        'test_auc': auc_score,
        'test_aupr': aupr_score,
        'test_f1': f1,
        'test_prec': prec,
        'test_rec': rec,
        'best_thr': best_thr
    }


# === æ ¸å¿ƒï¼šå‡†å¤‡åŸºç¡€æ•°æ®ï¼ˆåŸå§‹å›¾ã€ç²¾ç‚¼å›¾è¾¹ï¼Œå®Œå…¨ä¿ç•™åŸå§‹æµ‹è¯•é›†ï¼‰===
# 1. åŸå§‹å›¾è¾¹ï¼ˆBaselineç”¨ï¼‰
orig_adj_dense = to_dense_adj(data_full.edge_index.to(primary_device), max_num_nodes=N)[0]
edge_index_orig, _ = dense_to_sparse(orig_adj_dense)
edge_index_orig = edge_index_orig.to(primary_device)

# 2. ç²¾ç‚¼å›¾è¾¹ï¼ˆRefined Graphç”¨ï¼‰
orig_edge_set = set(map(tuple, edge_index_orig.t().cpu().numpy()))
orig_edge_count = edge_index_orig.size(1)
candidate_edges = []
candidate_weights = []

for i in range(edge_index_ensemble.size(1)):
    u, v = edge_index_ensemble[:, i].cpu().tolist()
    w = final_weights[i].item()
    # è·³è¿‡å·²å­˜åœ¨çš„è¾¹
    if (u, v) in orig_edge_set or (v, u) in orig_edge_set:
        continue
    # ç›¸ä¼¼åº¦é˜ˆå€¼è¿‡æ»¤
    if w <= 0.6:
        continue
    candidate_edges.append([u, v])
    candidate_weights.append(w)

# å¤„ç†å€™é€‰è¾¹ï¼ˆæ— å€™é€‰è¾¹åˆ™ç”¨åŸå§‹å›¾ï¼‰
if not candidate_edges:
    edge_index_train_refined = edge_index_orig
    edge_index_full_refined = edge_index_orig
else:
    candidate_edge_index = torch.tensor(candidate_edges, dtype=torch.long).t().contiguous().to(primary_device)
    candidate_edge_weight = torch.tensor(candidate_weights, dtype=torch.float).to(primary_device)

    # ç²¾ç‚¼å›¾ï¼ˆæ¨ç†ç”¨ï¼‰ï¼šåŸå§‹è¾¹ + æ–°å€™é€‰è¾¹
    max_total_new = int(orig_edge_count * 0.2)  # æœ€å¤šæ–°å¢20%è¾¹
    if candidate_edge_index.size(1) > max_total_new:
        topk_total = torch.topk(candidate_edge_weight, max_total_new)
        candidate_edge_index = candidate_edge_index[:, topk_total.indices]
    edge_index_full_refined = torch.cat([edge_index_orig, candidate_edge_index], dim=1)
    edge_index_full_refined, _ = coalesce(edge_index_full_refined, None, num_nodes=N)

    # è®­ç»ƒç”¨ç²¾ç‚¼å›¾ï¼šåç»­æŒ‰â€œæ–°è®­ç»ƒé›†èŠ‚ç‚¹â€è¿‡æ»¤
    edge_index_train_refined = edge_index_full_refined

# 3. æå–åŸå§‹è®­ç»ƒé›†èŠ‚ç‚¹ï¼ˆæ ¸å¿ƒï¼šä»…å¯¹è¿™äº›èŠ‚ç‚¹ç”¨4ä¸ªå‡½æ•°åˆ’åˆ†ï¼‰
original_train_nodes = torch.where(data_full.train_mask)[0].cpu().numpy()
# æå–åŸå§‹æµ‹è¯•é›†maskï¼ˆå…¨ç¨‹ä¸å˜ï¼‰
if isinstance(data_full.test_mask, np.ndarray):
    data_full.test_mask = torch.from_numpy(data_full.test_mask)
original_test_mask = data_full.test_mask.clone()

print(f"\n=== è®­ç»ƒé›†å†…éƒ¨åˆ’åˆ†é…ç½® ===")
print(f"åŸå§‹è®­ç»ƒé›†èŠ‚ç‚¹æ€»æ•°: {len(original_train_nodes)}")
print(f"æ¯ç§åˆ’åˆ†åæ–°è®­ç»ƒé›†èŠ‚ç‚¹æ•°: ~{int(len(original_train_nodes) * 0.8)}")
print(f"æ¯ç§åˆ’åˆ†åæ–°éªŒè¯é›†èŠ‚ç‚¹æ•°: ~{int(len(original_train_nodes) * 0.2)}")
print(f"åŸå§‹æµ‹è¯•é›†èŠ‚ç‚¹æ•°: {original_test_mask.sum().item()}ï¼ˆå…¨ç¨‹ä¸å˜ï¼‰")

# === æ ¸å¿ƒï¼šæ‰¹é‡ç”¨4ä¸ªå‡½æ•°åˆ’åˆ†â€œåŸå§‹è®­ç»ƒé›†èŠ‚ç‚¹â€ï¼Œå¹¶æµ‹è¯•æ€§èƒ½ ===
# å®šä¹‰4ç§åˆ’åˆ†æ–¹å¼ï¼ˆè¾“å…¥ï¼šåŸå§‹è®­ç»ƒé›†èŠ‚ç‚¹ + å…¨é‡ç‰¹å¾ï¼‰
split_functions = [
    ('SNV', lambda nodes: SNV_split(nodes, data_full.x)),
    ('GE', lambda nodes: GE_split(nodes, data_full.x)),
    ('METH', lambda nodes: METH_split(nodes, data_full.x)),
    ('CNA', lambda nodes: CNA_split(nodes, data_full.x))
]

# å­˜å‚¨æ‰€æœ‰ç»“æœ
all_results = []

# éå†æ¯ç§åˆ’åˆ†æ–¹å¼
for split_name, split_func in split_functions:
    print(f"\n" + "=" * 70)
    print(f"ğŸ“ æ­£åœ¨æµ‹è¯•ï¼šç”¨{split_name}ç‰¹å¾åˆ’åˆ†åŸå§‹è®­ç»ƒé›†èŠ‚ç‚¹")
    print(f"=" * 70)

    # æ­¥éª¤1ï¼šå¯¹åŸå§‹è®­ç»ƒé›†èŠ‚ç‚¹ï¼Œç”¨å½“å‰å‡½æ•°åˆ’åˆ†ä¸ºâ€œæ–°è®­ç»ƒé›†â€å’Œâ€œæ–°éªŒè¯é›†â€
    new_train_idx, new_val_idx = split_func(original_train_nodes)

    # æ­¥éª¤2ï¼šè½¬æ¢ä¸ºmaskï¼ˆé€‚é…æ¨¡å‹è¾“å…¥ï¼‰
    new_train_mask = torch.zeros(N, dtype=torch.bool, device=primary_device)
    new_train_mask[new_train_idx] = True
    new_val_mask = torch.zeros(N, dtype=torch.bool, device=primary_device)
    new_val_mask[new_val_idx] = True

    # æ­¥éª¤3ï¼šè®¡ç®—æ­£æ ·æœ¬æƒé‡ï¼ˆåŸºäºæ–°è®­ç»ƒé›†ï¼‰
    data_full.y = data_full.y.to(new_train_mask.device)
    train_labels = data_full.y[new_train_mask]
    pos_weight = (1 - train_labels.mean()) / train_labels.mean() if train_labels.mean() > 0 else torch.tensor(1.0,
                                                                                                              device=primary_device)

    # æ‰“å°å½“å‰åˆ’åˆ†çš„èŠ‚ç‚¹æ•°é‡ï¼ˆéªŒè¯æ­£ç¡®æ€§ï¼‰
    print(f"å½“å‰åˆ’åˆ†èŠ‚ç‚¹æ•°ï¼šæ–°è®­ç»ƒé›†={new_train_mask.sum().item()}, æ–°éªŒè¯é›†={new_val_mask.sum().item()}")

    # ---------------------- å®éªŒ1ï¼šåŸå§‹å›¾ï¼ˆBaselineï¼‰----------------------
    print(f"\nğŸ§ª å®éªŒ1ï¼šåŸå§‹å›¾æ¨¡å‹")
    data_full.y=data_full.y.to(primary_device)
    data_full.x=data_full.x.to(primary_device)
    edge_index_orig=edge_index_orig.to(primary_device)

    metrics_orig = train_and_evaluate(
        edge_index_orig, None,
        data_full.x, data_full.y,
        new_train_mask, new_val_mask, original_test_mask,  # å…³é”®ï¼šæ–°è®­ç»ƒ/éªŒè¯ï¼ŒåŸå§‹æµ‹è¯•
        pos_weight, primary_device,
        model_save_path=f'best_orig_{split_name}.pth'
    )
    all_results.append({
        'åˆ’åˆ†æ–¹å¼': split_name,
        'æ¨¡å‹ç±»å‹': 'åŸå§‹å›¾',
        'éªŒè¯é›†AUPR': metrics_orig['val_aupr'],
        'æµ‹è¯•é›†ACC': metrics_orig['test_acc'],
        'æµ‹è¯•é›†AUC': metrics_orig['test_auc'],
        'æµ‹è¯•é›†AUPR': metrics_orig['test_aupr'],
        'æµ‹è¯•é›†F1': metrics_orig['test_f1'],
        'æµ‹è¯•é›†Precision': metrics_orig['test_prec'],
        'æµ‹è¯•é›†Recall': metrics_orig['test_rec'],
        'æœ€ä¼˜é˜ˆå€¼': metrics_orig['best_thr']
    })
    print(f"âœ… åŸå§‹å›¾ | æµ‹è¯•é›†AUPR: {metrics_orig['test_aupr']:.4f}, F1: {metrics_orig['test_f1']:.4f}")

    # ---------------------- å®éªŒ2ï¼šç²¾ç‚¼å›¾ï¼ˆRefined Graphï¼‰----------------------
    print(f"\nğŸ§ª å®éªŒ2ï¼šç²¾ç‚¼å›¾æ¨¡å‹")
    # è¿‡æ»¤ç²¾ç‚¼å›¾è®­ç»ƒè¾¹ï¼šä»…ä¿ç•™â€œæ–°è®­ç»ƒé›†èŠ‚ç‚¹â€ä¹‹é—´çš„è¾¹ï¼ˆé¿å…æ•°æ®æ³„éœ²ï¼‰
    new_train_nodes_set = set(new_train_idx)
    u_list = edge_index_train_refined[0].cpu().tolist()
    v_list = edge_index_train_refined[1].cpu().tolist()
    u_in_train = torch.tensor([u in new_train_nodes_set for u in u_list], device=primary_device)
    v_in_train = torch.tensor([v in new_train_nodes_set for v in v_list], device=primary_device)
    train_edge_mask = u_in_train & v_in_train
    edge_index_train_current = edge_index_train_refined[:, train_edge_mask]

    metrics_refined = train_and_evaluate_a(
        edge_index_train=edge_index_train_current,
        edge_index_infer=edge_index_full_refined,
        x_full=data_full.x, y_full=data_full.y,
        new_train_mask=new_train_mask, new_val_mask=new_val_mask, original_test_mask=original_test_mask,
        pos_weight=pos_weight, device=primary_device,
        model_save_path=f'best_refined_{split_name}.pth'
    )
    all_results.append({
        'åˆ’åˆ†æ–¹å¼': split_name,
        'æ¨¡å‹ç±»å‹': 'ç²¾ç‚¼å›¾',
        'éªŒè¯é›†AUPR': metrics_refined['val_aupr'],
        'æµ‹è¯•é›†ACC': metrics_refined['test_acc'],
        'æµ‹è¯•é›†AUC': metrics_refined['test_auc'],
        'æµ‹è¯•é›†AUPR': metrics_refined['test_aupr'],
        'æµ‹è¯•é›†F1': metrics_refined['test_f1'],
        'æµ‹è¯•é›†Precision': metrics_refined['test_prec'],
        'æµ‹è¯•é›†Recall': metrics_refined['test_rec'],
        'æœ€ä¼˜é˜ˆå€¼': metrics_refined['best_thr']
    })
    print(f"âœ… ç²¾ç‚¼å›¾ | æµ‹è¯•é›†AUPR: {metrics_refined['test_aupr']:.4f}, F1: {metrics_refined['test_f1']:.4f}")

    # ---------------------- å®éªŒ3ï¼šé›†æˆé¢„æµ‹ï¼ˆEnsembleï¼‰----------------------
    print(f"\nğŸ§ª å®éªŒ3ï¼šé›†æˆæ¨¡å‹ï¼ˆåŸå§‹å›¾+ç²¾ç‚¼å›¾ï¼‰")
    # åŠ è½½ä¸¤ç§æ¨¡å‹
    model_orig = FullGraphGNN(in_dim=data_full.x.size(1), hidden_dim=64).to(primary_device)
    model_orig.load_state_dict(torch.load(f'best_orig_{split_name}.pth', map_location=primary_device))
    model_orig.eval()

    model_refined = FullGraphGNN(in_dim=data_full.x.size(1), hidden_dim=64).to(primary_device)
    model_refined.load_state_dict(torch.load(f'best_refined_{split_name}.pth', map_location=primary_device))
    model_refined.eval()

    with torch.no_grad():
        # ç”¨æ–°éªŒè¯é›†é€‰é˜ˆå€¼
        val_out_orig = model_orig(data_full.x, edge_index_orig)[new_val_mask]
        val_out_refined = model_refined(data_full.x, edge_index_full_refined)[new_val_mask]
        val_ens_logits = (val_out_orig + val_out_refined) / 2.0
        val_ens_probs = torch.sigmoid(val_ens_logits).cpu().numpy()
        val_ens_labels = data_full.y[new_val_mask].cpu().numpy()

        best_ens_thr = 0.5
        if val_ens_labels.sum() > 0:
            precision, recall, thresholds = precision_recall_curve(val_ens_labels, val_ens_probs)
            best_metric = 0.0
            for i, thr in enumerate(thresholds):
                if recall[i] >= 0.60:
                    f1_local = 2 * (precision[i] * recall[i]) / (precision[i] + recall[i] + 1e-8)
                    if f1_local > best_metric:
                        best_metric = f1_local
                        best_ens_thr = thr
            if best_metric == 0.0:
                best_ens_thr = 0.5

        # ç”¨åŸå§‹æµ‹è¯•é›†è¯„ä¼°
        test_out_orig = model_orig(data_full.x, edge_index_orig)[original_test_mask]
        test_out_refined = model_refined(data_full.x, edge_index_full_refined)[original_test_mask]
        test_ens_logits = (test_out_orig + test_out_refined) / 2.0
        test_ens_probs = torch.sigmoid(test_ens_logits).cpu().numpy()
        test_ens_labels = data_full.y[original_test_mask].cpu().numpy()

    # è®¡ç®—é›†æˆæ¨¡å‹æŒ‡æ ‡
    pred_ens = (test_ens_probs > best_ens_thr).astype(int)
    ens_acc = accuracy_score(test_ens_labels, pred_ens)
    ens_auc = roc_auc_score(test_ens_labels, test_ens_probs) if len(np.unique(test_ens_labels)) > 1 else 0.5
    ens_prec, ens_rec, _ = precision_recall_curve(test_ens_labels, test_ens_probs)
    ens_aupr = auc(ens_rec, ens_prec)
    ens_f1 = f1_score(test_ens_labels, pred_ens)
    ens_precision = precision_score(test_ens_labels, pred_ens) if pred_ens.sum() > 0 else 0.0
    ens_recall = recall_score(test_ens_labels, pred_ens)

    all_results.append({
        'åˆ’åˆ†æ–¹å¼': split_name,
        'æ¨¡å‹ç±»å‹': 'é›†æˆæ¨¡å‹',
        'éªŒè¯é›†AUPR': 0.0,  # é›†æˆæ¨¡å‹æ— å•ç‹¬éªŒè¯AUPR
        'æµ‹è¯•é›†ACC': ens_acc,
        'æµ‹è¯•é›†AUC': ens_auc,
        'æµ‹è¯•é›†AUPR': ens_aupr,
        'æµ‹è¯•é›†F1': ens_f1,
        'æµ‹è¯•é›†Precision': ens_precision,
        'æµ‹è¯•é›†Recall': ens_recall,
        'æœ€ä¼˜é˜ˆå€¼': best_ens_thr
    })
    print(f"âœ… é›†æˆæ¨¡å‹ | æµ‹è¯•é›†AUPR: {ens_aupr:.4f}, F1: {ens_f1:.4f}")

# === è¾“å‡ºæœ€ç»ˆæ±‡æ€»ç»“æœ ===
print(f"\n" + "=" * 120)
print("ğŸ“Š æœ€ç»ˆæ€§èƒ½æ±‡æ€»ï¼ˆä»…åˆ’åˆ†åŸå§‹è®­ç»ƒé›†ï¼ŒåŸå§‹æµ‹è¯•é›†ä¸å˜ï¼‰")
print("=" * 120)
df_results = pd.DataFrame(all_results)
# æŒ‰â€œåˆ’åˆ†æ–¹å¼â€å’Œâ€œæµ‹è¯•é›†AUPRâ€æ’åºï¼ˆæ¯ç§åˆ’åˆ†ä¸‹æœ€ä¼˜æ¨¡å‹åœ¨å‰ï¼‰
df_results_sorted = df_results.sort_values(by=['åˆ’åˆ†æ–¹å¼', 'æµ‹è¯•é›†AUPR'], ascending=[True, False])
print(df_results_sorted.to_string(index=False, float_format="%.4f"))

# ç»Ÿè®¡æ¯ç§åˆ’åˆ†æ–¹å¼ä¸‹çš„æœ€ä¼˜æ¨¡å‹ï¼ˆæŒ‰æµ‹è¯•é›†AUPRï¼‰
print(f"\n" + "=" * 80)
print("ğŸ† æ¯ç§åˆ’åˆ†æ–¹å¼ä¸‹çš„æœ€ä¼˜æ¨¡å‹ï¼ˆæŒ‰æµ‹è¯•é›†AUPRæ’åºï¼‰")
print("=" * 80)
best_per_split = []
for split_name in ['SNV', 'GE', 'METH', 'CNA']:
    split_data = df_results[df_results['åˆ’åˆ†æ–¹å¼'] == split_name]
    best_row = split_data.loc[split_data['æµ‹è¯•é›†AUPR'].idxmax()]
    best_per_split.append(best_row)

df_best = pd.DataFrame(best_per_split)
print(df_best[['åˆ’åˆ†æ–¹å¼', 'æ¨¡å‹ç±»å‹', 'æµ‹è¯•é›†AUPR', 'æµ‹è¯•é›†F1', 'æµ‹è¯•é›†ACC']].to_string(index=False,
                                                                                         float_format="%.4f"))

# ä¿å­˜ç»“æœåˆ°CSVï¼ˆä¾¿äºåç»­åˆ†æï¼‰
df_results_sorted.to_csv('train_subset_split_results.csv', index=False, float_format="%.4f")
print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ° 'train_subset_split_results.csv'")